# ViStreamASR Project Guide

## Table of Contents

1. [Introduction to ViStreamASR](#introduction-to-vistreamasr)
2. [Executive Summary](#executive-summary)
3. [Quick Start Usage](#quick-start-usage)
4. [System Architecture](#system-architecture)
5. [Component Overview](#component-overview)
6. [VAD Integration](#vad-integration)
7. [Core Processing Engine](#core-processing-engine)
8. [Streaming Interface](#streaming-interface)
9. [Command-Line Interface](#command-line-interface)
10. [Configuration and Tuning](#configuration-and-tuning)
11. [Performance Optimization](#performance-optimization)
12. [Integration Patterns](#integration-patterns)
13. [Troubleshooting and FAQ](#troubleshooting-and-faq)
14. [Advanced Topics](#advanced-topics)
15. [Related Documentation](#related-documentation)

---

## Introduction to ViStreamASR

ViStreamASR is a sophisticated streaming Automatic Speech Recognition (ASR) system designed for real-time Vietnamese speech recognition. Built on modern deep learning technologies, the system provides low-latency transcription capabilities with high accuracy, making it suitable for applications ranging from live captioning to voice assistants and meeting transcription.

### Key Features

- **Real-time Streaming**: Process audio in chunks with configurable latency (100-2000ms)
- **High Accuracy**: U2-based model with language model integration
- **Voice Activity Detection**: Advanced VAD filtering using Silero-VAD model
- **Multi-format Support**: WAV, MP3, FLAC, OGG, M4A and more
- **Cross-Platform**: Linux, macOS, and Windows support
- **GPU Acceleration**: Automatic GPU detection and utilization
- **Modular Architecture**: Clean separation of concerns for easy integration

### Target Applications

- **Live Captioning**: Real-time transcription for videos and streams
- **Voice Assistants**: Low-latency interaction capabilities
- **Meeting Transcription**: Automatic meeting minutes generation
- **Call Centers**: Call quality monitoring and transcription
- **Education**: Lecture transcription and captioning
- **Accessibility**: Real-time speech-to-text for hearing impaired

---

## Executive Summary

ViStreamASR represents a cutting-edge streaming ASR solution that successfully balances accuracy, latency, and computational efficiency. The system achieves:

- **Performance**: 0.3-0.4x Real-Time Factor (3-4x faster than real-time on CPU)
- **Accuracy**: State-of-the-art Vietnamese speech recognition with language model integration
- **Flexibility**: Configurable parameters for different use cases and requirements
- **Reliability**: Robust error handling and graceful degradation
- **Usability**: Multiple interfaces from CLI to programmatic API

### Technical Highlights

- **Architecture**: Modular component-based design with clear separation of concerns
- **Model**: U2 streaming architecture with bidirectional processing
- **Decoding**: Two-pass strategy combining n-gram beam search and CLM reranking
- **Optimization**: GPU acceleration, model caching, and memory-efficient processing
- **Interfaces**: CLI, streaming API, and extensible integration points

### Capabilities at a Glance

| Capability               | Description                                           | Performance            |
| ------------------------ | ----------------------------------------------------- | ---------------------- |
| **File Processing**      | Stream process audio files with partial/final results | RTF: 0.3-0.4x          |
| **Microphone Streaming** | Real-time speech recognition from audio devices       | Latency: 640ms default |
| **Model Management**     | Automatic download, caching, and loading              | Cache: ~2.7GB          |
| **Format Support**       | Multiple audio formats with automatic conversion      | Sample rate: 16kHz     |
| **GPU Acceleration**     | Optional GPU support for improved performance         | Speedup: 2-5x          |

---

## Quick Start Usage

### Installation

```bash
# Install dependencies
pip install torch torchaudio sounddevice numpy

# Install ViStreamASR
pip install vistream-asr
```

### Basic File Transcription

```bash
# Command line
vistream-asr transcribe audio.wav

# Python API
from streaming import StreamingASR

asr = StreamingASR()
for result in asr.stream_from_file("audio.wav"):
    if result['partial']:
        print(f"Partial: {result['text']}")
    if result['final']:
        print(f"Final: {result['text']}")
```

### Real-time Microphone Transcription

```bash
# Command line
vistream-asr microphone --duration 30

# Python API
asr = StreamingASR()
for result in asr.stream_from_microphone():
    print(f"Transcription: {result['text']}")
```

### System Information

```bash
# Check system status and configuration
vistream-asr info
```

---

## System Architecture

### High-Level Architecture

The ViStreamASR system follows a modular architecture with clear component separation:

```mermaid
graph TD
    A[Audio Sources] --> B[Streaming Interface]
    B --> C[Core Processing Engine]
    C --> D[Language Models]
    C --> E[Decoding Pipeline]
    C --> F[Output Management]

    subgraph "User Interfaces"
        G[CLI Interface]
        H[Streaming API]
        I[Integration Points]
    end

    subgraph "Core Components"
        J[ASREngine]
        K[IncrementalASR]
        L[Model Management]
    end

    G --> B
    H --> B
    I --> B
    B --> J
    J --> K
    J --> L
    K --> D
    K --> E
    J --> F
```

### Component Relationships

The system consists of four main components working together:

1. **Core Processing Component**: Handles ASR engine functionality, model management, and decoding
2. **Streaming Interface Component**: Provides high-level API for audio processing and result management
3. **CLI Interface Component**: Offers command-line tools for file and microphone processing
4. **Architecture Framework**: Defines overall system structure and deployment considerations

### Data Flow

Audio data flows through the system in the following sequence:

1. **Input**: Audio file or microphone input
2. **Preprocessing**: Format conversion, resampling, normalization
3. **Chunking**: Division into manageable audio segments
4. **Feature Extraction**: Conversion to acoustic features
5. **Incremental Processing**: Streaming recognition with context preservation
6. **Decoding**: Language model integration and hypothesis ranking
7. **Output**: Partial and final transcription results

---

## Component Overview

For detailed information about each component, refer to the specific component documentation:

- **[`COMPONENT_OVERVIEW.md`](COMPONENT_OVERVIEW.md)**: Comprehensive system overview
- **[`COMPONENT_CORE_PROCESSING_DOCS.md`](COMPONENT_CORE_PROCESSING_DOCS.md)**: Core processing implementation
- **[`COMPONENT_STREAMING_INTERFACE_DOCS.md`](COMPONENT_STREAMING_INTERFACE_DOCS.md)**: Streaming interface API
- **[`COMPONENT_CLI_INTERFACE_DOCS.md`](COMPONENT_CLI_INTERFACE_DOCS.md)**: Command-line interface reference

### Core Classes and Responsibilities

| Component               | Class/Function                      | Location                                     | Primary Responsibility         |
| ----------------------- | ----------------------------------- | -------------------------------------------- | ------------------------------ |
| **Core Processing**     | `ASREngine`                         | [`src/core.py:432`](src/core.py:432)         | Core ASR logic management      |
|                         | `IncrementalASR`                    | [`src/core.py:346`](src/core.py:346)         | Incremental audio processing   |
| **Streaming Interface** | `StreamingASR`                      | [`src/streaming.py:45`](src/streaming.py:45) | High-level streaming interface |
| **VAD Integration**     | `VADProcessor`                      | [`src/vad.py:16`](src/vad.py:16)             | Voice activity detection       |
|                         | `VADASRCoordinator`                 | [`src/vad.py:136`](src/vad.py:136)           | VAD-ASR coordination           |
| **CLI Interface**       | `transcribe_file_streaming()`       | [`src/cli.py:52`](src/cli.py:52)             | File transcription with VAD    |
|                         | `transcribe_microphone_streaming()` | [`src/cli.py:147`](src/cli.py:147)           | Real-time microphone with VAD  |

---

## VAD Integration

ViStreamASR now includes advanced Voice Activity Detection (VAD) capabilities using the Silero-VAD model, providing real-time speech filtering to improve computational efficiency and transcription accuracy.

### Key Features

- **Real-time Speech Detection**: Filter out silence periods automatically
- **Performance Optimization**: Reduce ASR processing load by 50-70%
- **Vietnamese Speech Support**: Optimized for Vietnamese tonal characteristics
- **Low Latency**: Processing time <1ms per 30ms audio chunk
- **Configurable Parameters**: Adjustable thresholds and durations

### Benefits

| Benefit                     | Description                               | Performance Impact                |
| --------------------------- | ----------------------------------------- | --------------------------------- |
| **Reduced Processing Load** | Only process speech segments, not silence | 50-70% fewer ASR calls            |
| **Improved Accuracy**       | Filter out background noise and silence   | Better signal-to-noise ratio      |
| **Lower Latency**           | Fewer ASR processing steps                | 20-30% faster transcription       |
| **Resource Efficiency**     | Reduced CPU and memory usage              | 40-60% lower resource consumption |

### Quick Start with VAD

#### Basic VAD Usage

```python
from vistreamasr.streaming import StreamingASR

# Enable VAD with default settings
asr = StreamingASR(
    model_name="tiny",
    use_vad=True  # Enable VAD
)

# Process audio with automatic speech filtering
for result in asr.stream_from_file("audio.wav"):
    if result['text']:
        print(f"Transcription: {result['text']}")
```

#### Advanced VAD Configuration

```python
# Optimized for Vietnamese speech
asr = StreamingASR(
    model_name="tiny",
    use_vad=True,
    vad_threshold=0.45,                    # Balanced for Vietnamese tones
    vad_min_speech_duration_ms=220,       # Shorter for monosyllabic words
    vad_min_silence_duration_ms=220,      # Conversational patterns
    vad_speech_pad_ms=25,                 # Preserve tonal transitions
    chunk_size_ms=640                      # Optimal chunk size
)
```

#### CLI Usage with VAD

```bash
# Basic VAD transcription
vistream-asr transcribe_file_streaming \
    --model-name tiny \
    --use-vad \
    audio.wav

# Optimized for Vietnamese speech
vistream-asr transcribe_file_streaming \
    --model-name tiny \
    --use-vad \
    --vad-threshold 0.45 \
    --vad-min-speech-duration-ms 220 \
    --vad-min-silence-duration-ms 220 \
    --vad-speech-pad-ms 25 \
    --language vi \
    vietnamese_audio.wav

# Live microphone with VAD
vistream-asr transcribe_microphone_streaming \
    --model-name tiny \
    --use-vad \
    --vad-threshold 0.4 \
    --vad-min-speech-duration-ms 200
```

### VAD Configuration Parameters

| Parameter                     | Default | Description                     | Recommended Range |
| ----------------------------- | ------- | ------------------------------- | ----------------- |
| `vad_threshold`               | 0.5     | Speech probability threshold    | 0.3-0.7           |
| `vad_min_speech_duration_ms`  | 250     | Minimum speech segment duration | 200-300ms         |
| `vad_min_silence_duration_ms` | 250     | Minimum silence duration        | 200-300ms         |
| `vad_speech_pad_ms`           | 30      | Padding around speech segments  | 20-50ms           |

### Performance Optimization with VAD

#### CPU Optimization

```python
# High-performance configuration for CPU
asr = StreamingASR(
    model_name="tiny",
    use_vad=True,
    vad_threshold=0.55,        # Higher threshold reduces false positives
    vad_min_speech_duration_ms=300,  # Longer segments reduce processing
    vad_min_silence_duration_ms=300,
    chunk_size_ms=640,
    compute_type="int8"       # Quantized models for CPU
)
```

#### Low-Latency Configuration

```python
# Ultra-low latency configuration
asr = StreamingASR(
    model_name="tiny",
    use_vad=True,
    vad_threshold=0.4,         # Lower threshold for faster detection
    vad_min_speech_duration_ms=200,  # Shorter segments
    vad_min_silence_duration_ms=200,
    vad_speech_pad_ms=20,      # Minimal padding
    chunk_size_ms=300          # Smaller chunks for lower latency
)
```

### Vietnamese Speech Optimization

The VAD system includes special optimizations for Vietnamese speech characteristics:

```python
# Northern Vietnamese (Hanoi dialect)
northern_config = {
    'use_vad': True,
    'vad_threshold': 0.42,
    'vad_min_speech_duration_ms': 200,
    'vad_min_silence_duration_ms': 200,
    'vad_speech_pad_ms': 25
}

# Southern Vietnamese (HCMC dialect)
southern_config = {
    'use_vad': True,
    'vad_threshold': 0.48,
    'vad_min_speech_duration_ms': 240,
    'vad_min_silence_duration_ms': 240,
    'vad_speech_pad_ms': 30
}
```

### Troubleshooting VAD Issues

#### VAD Not Detecting Speech

If VAD is missing speech segments:

```python
# Lower threshold for more sensitive detection
asr = StreamingASR(
    use_vad=True,
    vad_threshold=0.3,         # More sensitive detection
    vad_min_speech_duration_ms=150  # Shorter minimum duration
)
```

#### Too Many False Positives

If VAD is detecting too much background noise:

```python
# Increase threshold for stricter detection
asr = StreamingASR(
    use_vad=True,
    vad_threshold=0.7,         # Stricter detection
    vad_min_speech_duration_ms=400  # Longer minimum duration
)
```

#### Speech Segments Cut Off

If speech segments are being cut off prematurely:

```python
# Increase speech padding
asr = StreamingASR(
    use_vad=True,
    vad_speech_pad_ms=50,      # More padding preserves speech edges
    vad_min_silence_duration_ms=300  # Longer silence threshold
)
```

### Performance Benchmarks

| Configuration         | RTF Speedup | Processing Time  | Memory Reduction |
| --------------------- | ----------- | ---------------- | ---------------- |
| Without VAD           | 1.0x        | 100% baseline    | 0%               |
| With VAD (default)    | 2.1x        | ~48% of original | ~45%             |
| With VAD (optimized)  | 2.3x        | ~43% of original | ~55%             |
| With VAD (Vietnamese) | 2.2x        | ~45% of original | ~50%             |

### Integration Patterns

#### Web Application with VAD

```python
from vistreamasr.streaming import StreamingASR

class WebASRService:
    def __init__(self):
        self.asr = StreamingASR(
            model_name="tiny",
            use_vad=True,
            vad_threshold=0.45,
            chunk_size_ms=500
        )

    async def process_audio_stream(self, audio_stream):
        """Process streaming audio with VAD filtering."""
        async for audio_chunk in audio_stream:
            # Convert web audio format to numpy array
            chunk_data = self._convert_web_audio(audio_chunk)

            # Process with VAD-enabled ASR
            for result in self.asr.stream_from_file_data(chunk_data):
                yield result
```

#### Real-time Chat Application

```python
# Optimized for real-time chat with VAD
asr = StreamingASR(
    model_name="tiny",
    use_vad=True,
    vad_threshold=0.4,         # Fast detection
    vad_min_speech_duration_ms=150,  # Very short segments
    vad_min_silence_duration_ms=150,
    vad_speech_pad_ms=15,      # Minimal padding
    chunk_size_ms=300,         # Low latency
    auto_finalize_after=5.0    # Quick response
)
```

For detailed VAD implementation information, see [`COMPONENT_VAD_INTEGRATION_DOCS.md`](COMPONENT_VAD_INTEGRATION_DOCS.md).

---

## Core Processing Engine

The Core Processing Engine is the heart of ViStreamASR, responsible for all ASR functionality including model management, audio processing, and decoding.

### Key Features

- **Model Management**: Automatic download, caching, and loading of ASR models
- **Audio Processing**: Feature extraction and incremental processing
- **Decoding**: Two-pass strategy with language model integration
- **Performance Optimization**: GPU acceleration and memory management
- **Error Handling**: Robust exception handling and recovery

### Model Loading Process

The system implements sophisticated model management:

```python
# Automatic model loading with caching
def load_models(debug_mode=False):
    """Load models with automatic download and caching."""
    cache_dir = get_cache_dir()
    model_path = cache_dir / "pytorch_model.bin"

    if not model_path.exists():
        # Download with progress tracking
        response = requests.get(model_url, stream=True)
        # ... download logic

    # Load components
    acoustic_model = load_acoustic_model(model_path)
    language_models = load_language_models(model_path)
    decoders = initialize_decoders(language_models)
```

### Audio Processing Pipeline

Audio processing follows a multi-stage pipeline:

1. **Loading**: Load audio with torchaudio
2. **Conversion**: Stereo to mono, resampling to 16kHz
3. **Normalization**: Amplitude scaling to [-1, 1] range
4. **Feature Extraction**: 80-dimensional filter bank features
5. **Incremental Processing**: Context-preserving streaming recognition

### Decoding Strategy

The system uses a sophisticated two-pass decoding approach:

- **First Pass**: Fast n-gram beam search for initial results
- **Second Pass**: CLM beam ranking for improved accuracy
- **Bidirectional Processing**: Forward and backward encoder outputs
- **Hypothesis Reranking**: Dynamic reordering based on language model scores

For detailed information about the core processing engine, see [`COMPONENT_CORE_PROCESSING_DOCS.md`](COMPONENT_CORE_PROCESSING_DOCS.md).

---

## Streaming Interface

The Streaming Interface provides a high-level, easy-to-use API for streaming speech recognition functionality.

### Key Features

- **Simple API**: Intuitive interface for file and microphone streaming
- **Audio Format Handling**: Automatic conversion and normalization
- **Real-time Processing**: Live transcription from microphone input
- **Result Management**: Handling of partial and final transcription results
- **Error Resilience**: Comprehensive error handling and recovery

### File Streaming

Process audio files in streaming mode with real-time feedback:

```python
asr = StreamingASR(chunk_size_ms=640, auto_finalize_after=15.0)

for result in asr.stream_from_file("audio.wav"):
    if result['partial']:
        print(f"Partial: {result['text']}")
    if result['final']:
        print(f"Final: {result['text']}")
```

### Microphone Streaming

Real-time speech recognition from audio input devices:

```python
asr = StreamingASR()

try:
    for result in asr.stream_from_microphone():
        if result['partial']:
            print(f"\rPartial: {result['text']}", end='', flush=True)
        elif result['final']:
            print(f"\nFinal: {result['text']}")
except KeyboardInterrupt:
    print("\nStreaming stopped by user")
```

### Result Format

The streaming interface provides structured results:

```python
{
    'partial': True,        # Is this a partial result?
    'final': False,         # Is this a final result?
    'text': 'transcription text',  # The transcribed text
    'chunk_info': {         # Processing metadata
        'chunk_id': 1,
        'samples': 10240,
        'duration_ms': 640,
        'is_last': False
    }
}
```

For detailed information about the streaming interface, see [`COMPONENT_STREAMING_INTERFACE_DOCS.md`](COMPONENT_STREAMING_INTERFACE_DOCS.md).

---

## Command-Line Interface

The CLI Interface provides comprehensive command-line tools for file transcription and real-time microphone processing.

### Available Commands

#### `transcribe` - File Transcription

```bash
vistream-asr transcribe <audio_file> [options]
```

**Options:**

- `--chunk-size`: Chunk size in milliseconds (default: 640)
- `--auto-finalize-after`: Maximum duration before auto-finalizing segments (default: 15.0s)
- `--show-debug`: Enable debug logging

**Examples:**

```bash
# Basic transcription
vistream-asr transcribe audio.wav

# Low latency configuration
vistream-asr transcribe audio.wav --chunk-size 300

# Debug mode
vistream-asr transcribe audio.wav --show-debug
```

#### `microphone` - Real-time Microphone Transcription

```bash
vistream-asr microphone [options]
```

**Options:**

- `--duration`: Maximum recording duration in seconds
- `--chunk-size`: Chunk size in milliseconds
- `--auto-finalize-after`: Maximum duration before auto-finalizing segments
- `--show-debug`: Enable debug logging

**Examples:**

```bash
# Infinite recording (press Ctrl+C to stop)
vistream-asr microphone

# 30-second recording
vistream-asr microphone --duration 30

# Custom configuration
vistream-asr microphone --chunk-size 500 --show-debug
```

#### `info` - System Information

```bash
vistream-asr info
```

Displays library information including version, model status, cache location, and usage examples.

#### `version` - Version Information

```bash
vistream-asr version
```

Shows the current ViStreamASR version number.

### Output Formatting

The CLI provides rich, formatted output with visual indicators:

```
ðŸŽ¤ ViStreamASR File Transcription
==================================================
ðŸ“ Audio file: speech.wav
ðŸ“ Chunk size: 640ms
â° Auto-finalize after: 15.0s
ðŸ”§ Debug mode: False

ðŸŽµ Starting streaming transcription...
==================================================
ðŸ“ [PARTIAL   1]xin chÃ o tÃ´i lÃ 
âœ… [FINAL     1]xin chÃ o tÃ´i lÃ  john
--------------------------------------------------
ðŸ“ [PARTIAL   2]john Ä‘áº¿n tá»« hÃ  ná»™i
âœ… [FINAL     2]john Ä‘áº¿n tá»« hÃ  ná»™i
--------------------------------------------------
```

For detailed information about the CLI interface, see [`COMPONENT_CLI_INTERFACE_DOCS.md`](COMPONENT_CLI_INTERFACE_DOCS.md).

---

## Configuration and Tuning

### Key Configuration Parameters

| Parameter                                 | Default Value | Description              | Impact                         |
| ----------------------------------------- | ------------- | ------------------------ | ------------------------------ |
| `chunk_size_ms`                           | 640ms         | Audio chunk duration     | Latency vs accuracy trade-off  |
| `max_duration_before_forced_finalization` | 15.0s         | Maximum segment duration | Context vs responsiveness      |
| `decoding_chunk_size`                     | 16 frames     | Processing window size   | Computational efficiency       |
| `reverse_weight`                          | 0.3           | Bidirectional influence  | Accuracy vs speed              |
| `beam_size`                               | 100           | Beam search width        | Accuracy vs computational cost |
| `use_vad`                                 | False         | Enable VAD filtering     | Performance vs accuracy        |
| `vad_threshold`                           | 0.5           | VAD speech probability   | Sensitivity vs false positives |
| `vad_min_speech_duration_ms`              | 250           | Minimum speech duration  | Context vs responsiveness      |
| `vad_min_silence_duration_ms`             | 250           | Minimum silence duration | Segment boundaries             |
| `vad_speech_pad_ms`                       | 30            | Speech segment padding   | Edge preservation              |

### Configuration Examples

#### Low-Latency Configuration

```python
# For real-time chat applications
asr = StreamingASR(
    chunk_size_ms=300,      # 300ms chunks for lower latency
    auto_finalize_after=10.0, # Finalize segments after 10 seconds
    debug=False
)
```

#### High-Accuracy Configuration

```python
# For meeting transcription where accuracy is critical
asr = StreamingASR(
    chunk_size_ms=1000,     # 1000ms chunks for better context
    auto_finalize_after=30.0, # Longer segments for more context
    debug=True
)
```

#### GPU-Optimized Configuration

````python
# For GPU acceleration (automatic detection)
asr = StreamingASR(
    chunk_size_ms=640,
    debug=True  # Shows GPU/CPU usage
)

#### VAD-Optimized Configuration

```python
# For performance optimization with VAD
asr = StreamingASR(
    chunk_size_ms=640,
    use_vad=True,                    # Enable VAD filtering
    vad_threshold=0.5,               # Balanced speech detection
    vad_min_speech_duration_ms=250,  # Optimal for Vietnamese speech
    vad_min_silence_duration_ms=250,
    vad_speech_pad_ms=30,            # Preserve speech edges
    debug=True                       # Show performance metrics
)

#### VAD for Low-Latency Applications

```python
# Ultra-low latency with VAD optimization
asr = StreamingASR(
    chunk_size_ms=300,               # Smaller chunks for lower latency
    use_vad=True,                    # Enable VAD
    vad_threshold=0.4,               # More sensitive detection
    vad_min_speech_duration_ms=200,  # Shorter segments
    vad_min_silence_duration_ms=200,
    vad_speech_pad_ms=20,            # Minimal padding
    auto_finalize_after=5.0,         # Quick response
    debug=True
)
````

### Environment Variables

The system can be configured through environment variables:

| Variable                 | Default              | Description                |
| ------------------------ | -------------------- | -------------------------- |
| `VISTREAMASR_DEBUG`      | False                | Enable debug mode globally |
| `VISTREAMASR_CHUNK_SIZE` | 640                  | Default chunk size in ms   |
| `VISTREAMASR_CACHE_DIR`  | ~/.cache/ViStreamASR | Model cache directory      |

---

## Performance Optimization

### Performance Metrics

| Metric                     | Target Value     | Optimization Strategy                            |
| -------------------------- | ---------------- | ------------------------------------------------ |
| **Real-Time Factor (RTF)** | 0.3-0.4x         | GPU acceleration, chunk size optimization        |
| **Latency**                | 640ms default    | Chunk size reduction, pipeline optimization      |
| **Memory Usage**           | ~5GB             | Model quantization, batch processing             |
| **Accuracy**               | State-of-the-art | Language model integration, context preservation |
| **VAD Performance**        | 2.0-2.3x speedup | VAD filtering reduces processing load            |

### Optimization Strategies

#### Chunk Size Optimization

| Use Case              | Recommended Chunk Size | Latency  | Accuracy  |
| --------------------- | ---------------------- | -------- | --------- |
| Real-time chat        | 200-300ms              | Very low | Good      |
| Meeting transcription | 500-640ms              | Low      | Very good |
| High accuracy         | 1000-2000ms            | Medium   | Excellent |
| Low bandwidth         | 300-500ms              | Low      | Good      |

#### Memory Optimization

For large files or limited memory:

```bash
# Use smaller chunks to reduce memory usage
vistream-asr transcribe large_file.wav --chunk-size 400

# Process in segments for very large files
ffmpeg -i large_file.wav -f segment -segment_time 30 -c copy chunk_%03d.wav
for chunk in chunk_*.wav; do
    vistream-asr transcribe "$chunk"
done
```

#### VAD Optimization

Enable VAD for significant performance improvements:

```bash
# Enable VAD for 2x performance improvement
vistream-asr transcribe audio.wav --use-vad

# Optimize VAD parameters for specific use cases
vistream-asr transcribe audio.wav --use-vad --vad-threshold 0.4 --vad-min-speech-duration-ms 200

# VAD with memory optimization
vistream-asr transcribe large_file.wav --use-vad --chunk-size 400
```

```python
# VAD optimization for CPU-bound systems
asr = StreamingASR(
    use_vad=True,
    vad_threshold=0.6,        # Higher threshold reduces false positives
    vad_min_speech_duration_ms=300,  # Longer segments reduce processing
    vad_min_silence_duration_ms=300,
    compute_type="int8"       # Quantized models for CPU
)
```

#### GPU Acceleration

The system automatically detects and uses GPU acceleration:

```bash
# Check GPU availability
vistream-asr info

# GPU is used automatically if available
# No additional configuration required
```

### Performance Monitoring

Enable debug mode for performance monitoring:

```python
asr = StreamingASR(debug=True)
for result in asr.stream_from_file("audio.wav"):
    # Debug information includes processing time, RTF, memory usage
    pass
```

```bash
# Command line with debug output
vistream-asr transcribe audio.wav --show-debug
```

---

## Integration Patterns

### Web Application Integration

```python
from streaming import StreamingASR
import asyncio

class StreamingASRService:
    def __init__(self):
        self.asr = StreamingASR(chunk_size_ms=500, debug=False)
        self.is_processing = False

    async def process_audio_stream(self, audio_stream):
        """Process streaming audio from a web application."""
        self.is_processing = True

        try:
            async for audio_chunk in audio_stream:
                # Convert web audio format to numpy array
                chunk_data = self._convert_web_audio(audio_chunk)

                # Process with ASR engine
                result = self.asr.engine.process_audio(chunk_data, is_last=False)

                if result.get('current_transcription'):
                    yield {
                        'type': 'partial',
                        'text': result['current_transcription']
                    }

        finally:
            self.is_processing = False
```

### Desktop Application Integration

```python
import threading
from streaming import StreamingASR

class DesktopASRIntegration:
    def __init__(self):
        self.asr = StreamingASR(debug=True)
        self.transcription_callback = None

    def start_background_processing(self, audio_data):
        """Start ASR processing in a background thread."""
        def process_audio():
            try:
                for result in self.asr.stream_from_file(audio_data):
                    if self.transcription_callback:
                        self.transcription_callback(result)
            except Exception as e:
                print(f"Processing error: {e}")

        thread = threading.Thread(target=process_audio)
        thread.daemon = True
        thread.start()
        return thread
```

### Shell Script Integration

```bash
#!/bin/bash
# transcribe_script.sh

# Check if file exists
if [ ! -f "$1" ]; then
    echo "Error: File '$1' not found"
    exit 1
fi

# Transcribe file and save results
result=$(vistream-asr transcribe "$1" --chunk-size 500)
echo "$result" > transcription.txt

# Extract just the transcription text
grep "Complete Transcription:" transcription.txt | tail -1 | cut -d' ' -f3- > final_text.txt
```

### Pipeline Integration

```bash
# Process multiple files
for file in *.wav; do
    echo "Processing $file..."
    vistream-asr transcribe "$file" --chunk-size 300
done

# Process audio stream from another program
arecord -f cd -c 1 -r 16000 | vistream-asr microphone --duration 30
```

---

## Troubleshooting and FAQ

### Common Issues and Solutions

#### File Not Found Errors

**Problem:**

```bash
âŒ Error: Audio file not found: missing_file.wav
```

**Solution:**
Verify file path exists and is accessible. Check file permissions and path correctness.

#### Microphone Device Issues

**Problem:**

```bash
âŒ Error: No microphone devices found
```

**Solution:**

- Check microphone connections
- Verify permissions (especially on macOS/Linux)
- Install required dependencies: `pip install sounddevice`

#### Missing Dependencies

**Problem:**

```bash
âŒ Error: sounddevice library not installed. Install with: pip install sounddevice
```

**Solution:**
Install required dependencies:

```bash
pip install sounddevice torchaudio numpy torch
```

#### Model Download Issues

**Problem:**

```bash
âŒ Error: Failed to download model from https://...
```

**Solution:**

- Check internet connection
- Verify Hugging Face model access
- Clear cache: `rm -rf ~/.cache/ViStreamASR/`

### Performance Issues

#### High Latency

**Symptom:** Transcription results appear significantly delayed after speech.

**Solutions:**

```bash
# Reduce chunk size for lower latency
vistream-asr transcribe audio.wav --chunk-size 300

# Enable GPU acceleration (if available)
# System automatically detects and uses GPU
```

#### Memory Issues

**Symptom:** Program crashes with memory errors on large files.

**Solutions:**

```bash
# Use smaller chunk sizes to reduce memory usage
vistream-asr transcribe large_audio.wav --chunk-size 500

# Process in shorter segments for very long files
# Split file and process individually
```

#### Audio Quality Issues

**Symptom:** Poor transcription accuracy or garbled output.

**Solutions:**

```bash
# Check audio file integrity
# Ensure proper sample rate (16kHz recommended)
vistream-asr transcribe audio.wav --show-debug  # Check audio preparation
```

### Frequently Asked Questions

#### Q: What audio formats are supported?

**A:** The system supports WAV, MP3, FLAC, OGG, M4A and any other format supported by torchaudio. Audio is automatically converted to 16kHz mono for processing.

#### Q: How do I achieve the lowest latency?

**A:** Use the smallest chunk size that provides acceptable accuracy. For real-time chat applications, try 200-300ms chunks. Lower chunk sizes reduce latency but may decrease accuracy.

#### Q: Can I use the system without a GPU?

**A:** Yes, the system runs on CPU by default. GPU acceleration is optional and provides 2-5x performance improvement when available.

#### Q: How much memory does the system require?

**A:** The system requires approximately 5GB of RAM for model and processing buffers. For CPU-only usage, a multi-core processor is recommended.

#### Q: Can I process very long audio files?

**A:** Yes, the system processes audio in chunks, making it suitable for long files. For extremely large files, consider processing in segments to manage memory usage.

#### Q: Is the system suitable for production use?

**A:** Yes, the system includes comprehensive error handling, model caching, and performance optimizations suitable for production deployment.

#### Q: How do I customize the language model?

**A:** Currently, the system uses a pre-trained Vietnamese language model. Custom language model support is planned for future releases.

#### Q: Can I integrate this with my existing application?

**A:** Yes, the system provides multiple integration points including a streaming API, CLI tools, and extensible interfaces for web and desktop applications.

### Debug Mode Usage

Enable debug mode for troubleshooting:

```bash
# File processing with debug
vistream-asr transcribe audio.wav --show-debug

# Microphone processing with debug
vistream-asr microphone --show-debug
```

Debug information includes:

- Model loading status
- Audio file processing details
- Chunk processing information
- Buffer management status
- Performance metrics (RTF, processing time)
- Error stack traces when applicable

---

## Advanced Topics

### Custom Configuration

#### Configuration File Support

Create a `~/.vistreamasr.conf` file for persistent configuration:

```ini
[default]
chunk_size = 640
auto_finalize_after = 15.0
debug = false

[microphone]
default_duration = 30
chunk_size = 640

[file]
max_file_size = 100MB
supported_formats = wav,mp3,flac,m4a
```

#### Custom Themes and Formatting

The CLI supports customizable output formatting:

```python
# Custom color scheme (if terminal supports colors)
color_schemes = {
    'dark': {
        'partial': '\033[94m',    # Blue
        'final': '\033[92m',      # Green
        'error': '\033[91m',      # Red
        'reset': '\033[0m'
    },
    'light': {
        'partial': '\033[94m',    # Blue
        'final': '\033[92m',      # Green
        'error': '\033[91m',      # Red
        'reset': '\033[0m'
    }
}
```

### Batch Processing

#### Shell Script for Batch Processing

```bash
#!/bin/bash
# batch_transcribe.sh

for file in "$@"; do
    echo "Processing: $file"
    output_file="${file%.*}.txt"
    vistream-asr transcribe "$file" > "$output_file"
    echo "Results saved to: $output_file"
done
```

#### Python Batch Processing

```python
from streaming import StreamingASR
import os

def batch_transcribe_directory(directory, output_dir="output"):
    """Transcribe all audio files in a directory."""
    os.makedirs(output_dir, exist_ok=True)
    asr = StreamingASR()

    for filename in os.listdir(directory):
        if filename.endswith(('.wav', '.mp3', '.flac')):
            input_path = os.path.join(directory, filename)
            output_path = os.path.join(output_dir, f"{os.path.splitext(filename)[0]}.txt")

            print(f"Processing: {filename}")
            with open(output_path, 'w') as f:
                for result in asr.stream_from_file(input_path):
                    if result.get('text'):
                        f.write(result['text'] + '\n')
```

### Quality Assessment

#### Word Error Rate Calculation

```bash
# Compare with reference text
reference="reference.txt"
transcription="output.txt"

# Calculate word error rate (requires wer-calc tool)
wer-calc "$reference" "$transcription"

# Or use simple word count comparison
ref_words=$(wc -w < "$reference")
trans_words=$(wc -w < "$transcription")
echo "Reference words: $ref_words"
echo "Transcription words: $trans_words"
```

#### Custom Output Formats

Extract specific information from CLI output:

```bash
# Extract just the final transcription
vistream-asr transcribe audio.wav | \
    grep "Complete Transcription:" | \
    tail -1 | \
    cut -d' ' -f3- > clean_transcription.txt

# Extract processing statistics
vistream-asr transcribe audio.wav | \
    grep "Processing time:" | \
    cut -d' ' -f3 | \
    tr -d 's' > processing_time.txt
```

### System Service Integration

Create a systemd service for continuous transcription:

```ini
# /etc/systemd/system/vistreamasr.service
[Unit]
Description=ViStreamASR Transcription Service
After=sound.target

[Service]
Type=simple
User=asr
ExecStart=/usr/local/bin/vistream-asr microphone --duration 3600
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
```

### Model Management

#### Cache Management

The system automatically manages model caching:

- **Cache Location**: `~/.cache/ViStreamASR/`
- **Cache Size**: ~2.7GB for the complete model
- **Automatic Updates**: Models are cached to avoid repeated downloads
- **Cache Clearing**: Manual cache clearing when needed

#### Manual Cache Management

```bash
# Check cache location
vistream-asr info

# Clear cache (useful for model updates)
rm -rf ~/.cache/ViStreamASR/

# Check cache usage
du -sh ~/.cache/ViStreamASR/
```

---

## Related Documentation

For detailed information about specific components, refer to the following documentation:

### Component Documentation

- **[`COMPONENT_OVERVIEW.md`](COMPONENT_OVERVIEW.md)**: Comprehensive system overview, component relationships, and technical specifications
- **[`COMPONENT_CORE_PROCESSING_DOCS.md`](COMPONENT_CORE_PROCESSING_DOCS.md)**: Core processing engine implementation, model management, audio processing, and decoding mechanics
- **[`COMPONENT_STREAMING_INTERFACE_DOCS.md`](COMPONENT_STREAMING_INTERFACE_DOCS.md)**: Streaming interface API, file and microphone processing, audio preprocessing, and error handling
- **[`COMPONENT_CLI_INTERFACE_DOCS.md`](COMPONENT_CLI_INTERFACE_DOCS.md)**: Command-line interface reference, usage examples, troubleshooting, and integration patterns
- **[`COMPONENT_VAD_INTEGRATION_DOCS.md`](COMPONENT_VAD_INTEGRATION_DOCS.md)**: VAD integration implementation, API reference, configuration options, and Vietnamese speech optimization

### Architecture Documentation

- **[`ARCHITECTURE.md`](ARCHITECTURE.md)**: Detailed system architecture, component design, technical specifications, deployment considerations, and design decisions

### Source Code References

Key source files and their locations:

- **[`src/core.py`](src/core.py)**: Core ASR engine, model management, and processing logic
- **[`src/streaming.py`](src/streaming.py)**: High-level streaming interface and audio processing
- **[`src/cli.py`](src/cli.py)**: Command-line interface implementation
- **[`example.py`](example.py)**: Usage examples and demonstrations
- **[`main.py`](main.py)**: Main application entry point

### Additional Resources

- **README.md**: Project overview and quick start guide
- **requirements.txt**: Python dependencies
- **setup.py**: Package installation configuration
- **pyproject.toml**: Project metadata and build configuration

---

## Summary

ViStreamASR provides a comprehensive, modular streaming ASR system that successfully balances accuracy, latency, and computational efficiency. The system's component-based architecture allows for flexible integration and easy extension while maintaining high performance and reliability.

### Key Strengths

- **Real-time Performance**: Achieves 0.3-0.4x RTF with configurable chunk sizes
- **High Accuracy**: U2-based model with sophisticated language model integration
- **User-friendly Interfaces**: Multiple interfaces from CLI to programmatic API
- **Robust Error Handling**: Comprehensive exception handling and graceful degradation
- **Cross-platform Support**: Linux, macOS, and Windows compatibility
- **GPU Acceleration**: Automatic GPU detection and utilization

### Getting Started

1. **Installation**: Install dependencies and the package
2. **Basic Usage**: Try the CLI commands for quick testing
3. **API Integration**: Use the streaming interface for custom applications
4. **Configuration**: Tune parameters for specific use cases
5. **Optimization**: Apply performance tuning for better results

### Next Steps

- Explore the component documentation for detailed implementation information
- Try the examples in the `example.py` file
- Experiment with different configuration parameters
- Integrate ViStreamASR into your applications
- Join the community for support and feature requests

For the latest updates and additional resources, refer to the project repository and documentation links above.

---

_This project guide provides a comprehensive overview of ViStreamASR. For component-specific details, refer to the individual component documentation linked throughout this guide._
